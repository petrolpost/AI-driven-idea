[tool:pytest]
# Pytest configuration for Interactive Data Dashboard

# Test discovery
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*

# Minimum version requirements
minversion = 6.0

# Add options
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --cov=src/interactive_data_dashboard
    --cov-report=term-missing
    --cov-report=html:htmlcov
    --cov-report=xml:coverage.xml
    --cov-fail-under=80
    --durations=10
    --color=yes
    -ra

# Markers for test categorization
markers =
    unit: Unit tests
    integration: Integration tests
    performance: Performance tests
    slow: Slow running tests
    ui: User interface tests
    data: Data processing tests
    chart: Chart generation tests
    mock: Tests that use mocking
    regression: Regression tests
    smoke: Smoke tests for basic functionality
    critical: Critical functionality tests
    optional: Optional feature tests

# Test timeout (in seconds)
timeout = 300

# Warnings configuration
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::FutureWarning
    error::pytest.PytestUnraisableExceptionWarning

# Logging configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

# File logging
log_file = tests/logs/pytest.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Capture configuration
console_output_style = progress

# Test collection
collect_ignore = [
    "setup.py",
    "build",
    "dist",
    ".tox",
    ".git",
    "__pycache__",
    "*.egg-info"
]

# Doctest configuration
doctest_optionflags = NORMALIZE_WHITESPACE IGNORE_EXCEPTION_DETAIL ELLIPSIS

# Cache configuration
cache_dir = .pytest_cache

# Parallel execution (if pytest-xdist is installed)
# Uncomment the following line to enable parallel test execution
# addopts = -n auto

# Performance testing (if pytest-benchmark is installed)
benchmark-only = false
benchmark-sort = mean
benchmark-group-by = group
benchmark-timer = time.perf_counter
benchmark-disable-gc = true
benchmark-skip = false

# Coverage configuration
[coverage:run]
source = src/interactive_data_dashboard
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */.*
    */venv/*
    */env/*
    setup.py
    conftest.py

[coverage:report]
exclude_lines =
    pragma: no cover
    def __repr__
    if self.debug:
    if settings.DEBUG
    raise AssertionError
    raise NotImplementedError
    if 0:
    if __name__ == .__main__.:
    class .*\bProtocol\):
    @(abc\.)?abstractmethod

show_missing = true
skip_covered = false
skip_empty = false
precision = 2

[coverage:html]
directory = htmlcov
title = Interactive Data Dashboard Coverage Report

[coverage:xml]
output = coverage.xml